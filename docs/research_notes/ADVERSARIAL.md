# ADVERSARIAL
adversarial protocol for adversarial network training

## Requirements
- Store functions to compute robustness and performance under adversarial perturbations, and use for robustness certification and verification in `src.verification`. The functions will be called given the object which will generate a sub-class with respect to bounds and convex optimization.

## RESEARCH
- Adversarial examples are a class of attack against ML models, studied particularly on deep neural networks for multiclass image classification. The attacker constructs a small change to a given, fixed input, that wildly changes the predicted output. Notationally, if the input is x, we denote an adversarial version of that input by x + α, where α is the change or perturbation introduced by the attacker. When x is a vector of pixels (for images), then xi is the i’th pixel in the image and αi is the change to the i’th pixel.
- Add *perturbation_layer* which distorts each pixel by some perturbation epsilon norm. How do adversarial perturbations affect the momentum (optimize convergence for gradient descent), weight decay regularization, model overfitting (possibly, but generally not concerned given scale of current models used)
- The noise is drawn from a Laplace or Gaussian distribution and its standard deviation is proportional to: (1) L, the p-norm attack bound for which we are constructing the network and (2) ∆, the sensitivity of the pre-noise computation (the grey box in Fig. 1(a)) with respect to p-norm input changes.
- **In our method, we have adapted the integration of an FGSM and PGD attack (where model gradients are accessed, and FGSM will distort each pixel by some norm $p$.**
- Why is the perturbation done early in the network? Because, we find that this approach of perturbing the features in the hidden layers (e.g. ReLU, FCN, Dense) for sensitivity analysis is difficult to generalize. Combining bounds in this way leads to looser and looser approximations. Moreover, layers such as batch normalization [28], which are popular in image classification networks, do not appear amenable to such bounds (indeed, they are assumed away by some previous defenses [12]). Thus, our general recommendation is to add the DP noise layer early in the network – where bounding the sensitivity is easy – and taking advantage of DP’s post-processing property to carry the sensitivity bound through the end of the network. We are applying "bounded norm adversarial perturbations"
- APPROACH_LIST = ['PGD', 'IBP', 'FastLin', 'MILP', 'PercySDP', 'ZicoDual', 'CROWN', 'CROWN-IBP', 'LPAll' 'Diffai', 'RecurJac', 'FastLip']
- Let Nθ : R d → R k be a neural network with d input features and k output classes, parameterized by weights θ. The network Nθ assigns the class i ∈ {1, . . . , k} to the point x ∈ R d if Nθ(x)i > Nθ(x)j for all j 6= i. Let B(x) denote the `∞-ball of radius  around a point x ∈ R d . A network Nθ is called -robust around a point x ∈ R d if Nθ assigns the same class to all points x˜ ∈ B(x). Note that this norm ball is just a vector that is applying matmul() for each layer-to-layer computation
