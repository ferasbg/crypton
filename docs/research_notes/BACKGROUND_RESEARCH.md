# BACKGROUND RESEARCH
Some notes I took for my background research.

## Intriguing properties of neural networks
- However, we find that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily change the network’s prediction (see figure 5). These perturbations are found by optimizing the input to maximize the prediction error. We term the so perturbed examples “adversarial examples”
- These results suggest that the deep neural networks that are learned by backpropagation have nonintuitive characteristics and intrinsic blind spots, whose structure is connected to the data distribution
in a non-obvious way.

## SCALABLE TRAINING OF NEURAL NETWORKS GIVEN STL SPECIFICATIONS
- We consider the problem of learning a trace-valued function f θ to verifiably satisfy a specification of the form ∀x ∈ S. (f θ (x), 0) |= φ where x denotes an input ranging over an input space S, and f θ (x) is the trace generated by f when evaluated on input x, θ represents the trainable parameters of f , and φ is an STL specification. Formally, our problem statement is as follows
- for an STL specification φ, its quantitative semantics can be used to construct a function ρ(φ, f (x), t) whose scalar valued output is such that ρ(φ, f (x), t) ≥ 0 ⇐⇒ (f (x), t) |= φ (Donzé & Maler, 2010). In terms of the quantitative semantics, the verification problem is equivalent to showing that ∀x ∈ S. ρ(φ, f (x), 0) ≥ 0. This verification task can be written as the optimization problem of finding the sequence of inputs x such that the sequence of outputs f (x) result in the strongest violation of the specification with regard to the quantitative semantics: min ρ(φ, f (x), 0) subject to x ∈ S.

## A Provable Defense for Deep Residual Networks 
- "Abstract Interpretation is a technique for verifying properties of programs by soundly overapproximating their behavior. When applied to neural networks, an infinite set (a ball) of possible inputs is passed to an approximating "abstract" network to produce a superset of the possible outputs from the actual network. Provided an appropreate representation for these sets, demonstrating that the network classifies everything in the ball correctly becomes a simple task. The method used to represent these sets is the abstract domain, and the specific approximations are the abstract transformers."

## Rediscovering Deep Neural Networks in Finite-State Distributions
- "In conventional image acquisition devices, the intensity of pixels can be interpreted as the probability of presence of photons in a spatial position and some wavelength. Therefore, a single image is considered as the distribution of photons on the spatial plane with finite states when the number of pixels is finite." (Marvasti et. al, 2018)
- The ever-increasing complexity of Convolutional Neural Networks (CNN) and their associated set of layers demand deeper insight into the internal mechanics of CNNs. The functionality of CNNs is often understood as a series of projections and a variety of non-linearities to increase the capacity of the model (Ramachandran, Zoph, and Le 2017, Zheng et. al 2015).
- Despite the fact that the prediction layer of CNNs (e.g., the Softmax layer) and the loss functions (e.g., Cross Entropy) are borrowed from the Bayesian framework, a clear connection of the functionality of the intermediate layers with probability theory remains elusive. The current understanding of CNNs leaves much to subjective designs with extensive experimental justifications.
- To demonstrate an example of interpreting real-valued data as uncertain realizations, consider a set of m -pixel RGB image data. We can view each pixel as being generated from the set { R , G , B } and further interpret the value of each channel as the unnormalized log-probability of being in the corresponding state.

## Skip Connections for Semantic Segmentation
- Upsampling using transposed convolutions or unpooling loses information, and thus produces coarse segmentation. Skip connections allow us to produce finer segmentation by using layers with finer information. 4 Skip connections combine the coarse final layer with finer, earlier layers to provide local predictions that “respect” global positions.

## FRAGMENTED NAPKIN RESEARCH
- computational complexity and other design replication metadata
- GPU usage for parallelization of task, or just local
- signed 32-bit integers for defining types for variables relating to computing each respective function part of each class of a module, the integers themselves represent some member variable like $$\epsilon$$, weight_decay, momentum (optimize local minima convergence), checking neuron states, etc.
- secure computational graph representation of tf.Network

## langrangian duality and convex optimization applied to the context of constraint-satisfaction problems
- setup a policy and then reduce the execution state to a constraint-satisfaction problem

## soundness of verification algorithms with respect to complete-ness
- "The verification algorithms that we survey are sound, meaning that they will only report that a property holds if the property actually holds. Some of the algorithms that we discuss are also complete, meaning that whenever the property holds, the algorithm will correctly state that it holds", noted from the "Algorithms for Verifying Deep Neural Networks" paper.

## robustness variables for neural network abstraction
- epsilon (l2-norm bounded ball), perturbation_factor, input_set, output_set,  

## some notes on sequential computation
- noticing that some nominal techniques for optimizing computational efficiency is by the means of not doing composite computations (nested method calls), albeit this applies to large-scale, generally NP-complete algorithms.
- Well..... this also has to do with the product of each time complexity given either n iterations of a loop or the number of states to access or check and/or track    

## formal math related questions
- how do we ensure the proof is valid in terms of not being quack or lacking robustness in its formalism design? This is super important to consider.
- the high-level: implementing verification algo for each trace property (specification / standard), input params required and modified network state variables (e.g. compute upper / lower bound)

## norms and perturbations
- l2-norm is the square root of the sum of the squared scalar values in the matrix
- $$l\infin$$ norm is the largest absolute value of the vector, assuming that each matrix has been downsampled already
- imagine the input_image is decoupled into vectors and then you are essentially adding distortions by the means of loss compression (default), pixelwise uncorrelated gaussian noise, and undetected adversarial perturbations 

## General Notes of Formal Verification
- A fundamental component of neural network verification is the computation of bounds on the values their outputs can take. 
- Verification algorithms fall into three categories: unsound (some false properties are proven false), incomplete (some true properties are proven true), and complete (all properties are correctly verified as either true or false).
- A critical component of the verification systems developed so far is the computation of lower and upper bounds on the output of neural networks when their inputs are constrained to lie in a bounded set. There is also a lot of unintended and counter-intuitive neural network behavior that must be accounted for with respecting to understanding the state to work with.
- In incomplete verification, by deriving bounds on the changes of the prediction vector under restricted perturbations, it is possible to identify safe regions of the input space
- In complete verification, bounds can also be used as essential subroutines of Branch and Bound complete verifiers (Bunel et al., 2018). Finally, bounds might also be used as a training signal to guide the network towards greater robustness and more verifiability (Gowal et al., 2018, Mirman et al., 2018, Wong and Kolter, 2018).
- In this setting, our goal is to verify whether a network is robust to `∞ norm perturbations of radius $$\epsilon_{verif}$$ 
- In order to do so, we search for a counter-example (an input point for which the output of the network is not the correct class).
- If the minimum is positive, we have not succeeded in finding a counter-example, and the network is robust. We can assume our error rate $$\epsilon  > 0$$.
- What I am unclear with is input_params, and the optimization state of the BoundedNetwork 
- "Symbolic interval analysis is a formal analysis method for certifying the robustness of neural networks. Any safety properties of neural networks can be presented as a bounded input range, a targeted network, and a desired output behavior. Symbolic interval analysis will relax the network to an interval-based version such that it can directly take in arbitrary input interval and return an output interval. Such an analysis is sound, as it will always over-approximate the ground-truth output range of network within the given input range. Also, to make the estimations more accurate, the dependencies of the network inputs can be kept as a symbolic interval such that our interval-based network can propagate it layer by layer and return an output symbolic interval. The output interval/symbolic interval can then be used to verify the safety properties."


## SPECIFYING THE PROBLEM WE ARE SOLVING
- what are we converting into an abstract interpretation / symbolic representation?
- What input states even matter to symbolize and then convert into a propositional logic statement? How do we handle complexity relating to the inpt-output relationships between the input/outputs? 
- What algorithms are we implementing and what can we throw away?
- What is our main initial focus in terms of an MVP, and what should we iterate on afterward?
- the mvp for now should focus on right now should be abstract interpretation / symbolic abstraction via bounded model checking to check safety / robustness trace properties defined in temporal logic specifications (logical statements to compute on abstraction of network state), while the network itself is encrypted, tf.compat_v1, tensorflow downgraded, tf_encrypted enabled network.
- The iterations afterward should be to enhance the methods that are used to optimize the state of soundedness.
- What metrics am I evaluating and how will I compute those metrics?
- How will I incorporate numpy and scipy and sympy to help me with the mathematical formulation and maths in general that I need to write?
- How are we exactly implementing tf-encrypted and should we downgrade our tensorflow with tf.compat_v1
- Given conv layer params are field size, channels,stride, padding for `Conv2d(field size, channels,stride, padding)`
- assume runtime cost to be greater given encrypted computations as well as computational cost, but minimal or close to plaintext / public computations
- bounded model checking for stl specifications for safety and robustness verification of secure, private deep convolutional neural network
- when they ask for redesign and retest, how are you exactly evaluating this part? with regards to how you improved the algorithms / program overall in terms of computational cost, performance, privacy, safety, and convergence to soundedness of formalisms?
- it's given that any formal specification is meant to inductively check whether a safety property state is true or not for the sake of optimizing the system's state, and that having a property to verify the system's behavior against adversaries isn't enough to get rid of the problem, but rather gauge visibility and to then formulate that fault as an optimization problem to converge the nodes/components of the system to interoperably function
- there can be many improvements in terms of evaluating the encrypted state and with respect to distributed parallelization if the trusted environment isn't local (like my computer vs. distributed computers in a warehouse)
- state some nominal optimizations that were done to enhance adversarial capability

## FORMAL NOTES GIVEN BACKGROUND RESEARCH

## Context and Background
- To overview the high-level idea with the interpretability and lack of formal reasoning for autonomous systems, the Stanford Center for AI Safety states: "A key difficulty, however, is that we are currently unable to reason about AI systems. Indeed, we understand quite well the algorithms used for training them — this topic has been studied extensively — but, given a trained AI system, we have no way to make rigorous claims about its behavior. In classical, imperative programing one can often look at and reason about the code, write invariants, and prove certain properties of the system (either manually or automatically). Because such code is written by humans, good software engineering practices coupled with formal methods can ensure that it is also guaranteed to perform as expected. In machine-learned systems, however, the program amounts to a highly complex mathematical formula for transforming inputs into outputs. Humans can barely parse the formulas defining these systems, let alone reason about them. And off-the-shelf formal tools are so far able to reason about only very small instances of such systems. Currently, we have little recourse but to blindly trust that the training algorithms were sufficiently “clever” and have produced a system that is correct. However, if we are to use AI components in safety-critical systems, this situation is unsatisfactory."
- Even if there is complete security and privacy of the neural networks themselves, there aren't provable guarantees of the privacy properties as well as the safety properties with regards to the convolutional neural network architecture and behavior itself.


## Deep Convolutional Neural Networks
- According to Reluplex, that implements an automated theorem prover in order to iterate and verify the deep neural network properties, they note that "a deep neural network is comprised of a set of layers of nodes, and the value of each node is determined by computing a linear combination of values from nodes in the preceding layer and then applying an activation function to the result. These activation functions are non-linear and render the problem non-convex."

## Secure Multi-Party Computation 
- Given that there's [[semi-honest]] parties computing arithmetic shares to securely aggregate and compute over the composite function $$f(x)$$ for $$f(x) = N$$ for $$N$$ is the neural network.

## Formal Specifications
- Stanford AI Safety Research et. al state that the urgency to the question: if no failure has been found, how sure are we that the system is safe? This will require the development of algorithms that have formal or probabilistic guarantees of convergence. Scalability also remains a significant challenge. Autonomous systems can encounter a wide range of complex interactions, so safety validation algorithms must be able to efficiently discover failures in the most complex scenarios. The algorithms presented in this survey are a promising step toward safe and beneficial autonomy.

## Symbolic Interval Analysis
- Wang S. states: "Symbolic interval analysis is a formal analysis method for certifying the robustness of neural networks. Any safety properties of neural networks can be presented as a bounded input range, a targeted network, and a desired output behavior. Symbolic interval analysis will relax the network to an interval-based version such that it can directly take in arbitrary input interval and return an output interval. Such an analysis is sound, as it will always over-approximate the ground-truth output range of network within the given input range. Also, to make the estimations more accurate, the dependencies of the network inputs can be kept as a symbolic interval such that our interval-based network can propagate it layer by layer and return an output symbolic interval. The output interval/symbolic interval can then be used to verify the safety properties."

